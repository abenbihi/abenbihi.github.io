<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <meta name="generator" content="Pelican" />
        <title>Box Matching</title>
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <meta name="Description" content="Assia Benbihi, personal website">
       
        <link rel="stylesheet"
              href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
              integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T"
              crossorigin="anonymous">
        <link rel="stylesheet"
              href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css"> 

        <link rel="stylesheet"
              href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

        <style>
@import url(https://fonts.googleapis.com/css?family=Raleway:400,700);
body, input, select, textarea {font-family: "Raleway", Arial, Helvetica, sans-serif; font-size: 12pt; font-weight: 400; line-height: 1.6em;}
a			{color: #4285F4}
nav a             {color: white}
nav a:hover       {color: white}
h4                {color: #1a60a2}
h5                {color: #1a60a2}
strong	        {color: #1a60a2; font-weight:400}
ol.pubs		{list-style-type: decimal; padding-left: 2em}
ul.pub		{list-style:none; margin-bottom:1em}
ul.pub		{list-style:none; margin-bottom:1em}
ul.pub li.title	{color: #333333}
ul.pub li.authors {color: #555555}
ul.pub li.venue	{font-style: italic}
ul.pub li.venue a {text-decoration: none}
.header           {background-color:#DADADA; margin-bottom:5px}
.card             {border:none}
.container         {margin-top: 1em}
:target {display: block; position: relative; top: -120px; visibility: hidden;}
        </style>

        <meta name="description" content="Finding local correspondences among cluttered scenes requires identifying the image's regions to match. Current approaches based on saliency or..." />
</head>


<body id="index" class="home">
<section id="content" class="body">
  <article>
  <div class="container">
    <div class="row">
      <div class="col">
        <div class="media">
          <div class="media-body">
            <center><h1 class="sticky-top" style="color:#4285F4;">
                Box Matching</h1></center>
          </div>
        </div>
      </div>
    </div>

    <div class="entry-content">
      <p>Finding local correspondences among cluttered scenes requires identifying the
image's regions to match. Current approaches based on saliency or semantics
provide only coarse and few such regions per image. I propose to identify
regions to match at a finer level, for example at the level of an object.
Matching these finer regions allows computing a coarse image transformation
later used to constrain the local correspondences. </p>
<p>Current experiments involve the Phototourism dataset where regions of interest
are doors, windows, statues ... Manual annotations of such regions are
collected in the form of semantic boxes. The annotation effort is reduced by
taking advantage of the dense although incomplete 3d information of the
dataset. These labels are used to train an existing object detector (Faster
R-CNN). In parallel, the region matching is developed using the manual
annotation first. Regions are matched according to their semantics and visual
similarity in an N-to-N fashion.</p>
<p align="center">
<img width="1024" src="/images/box_matching/selected_match.png">
</p>

<p align="center">
An example of semantic boxes on the Phototourism dataset and four putative box matches.
</p>

<h4>Results</h4>
<p>Experiments aim at showing the benefits of the two-stages matching against a
naive matching for stereo-matching and localization. Current results are
computed on the validation set of the phototourism dataset with manual box
annotations. This will be updated to integrate the trained box detector.</p>
<h5>Stereo Matching</h5>
<p>Given a pair of images of the same scenes, the goal is to recover the camera
displacement.</p>
<p>I measure the accuracy of the camera displacement between image pairs of the
same scene, as in the
<a href="thehttps://vision.uvic.ca/image-matching-challenge/">image-matching-challenge</a>.
Given a set of angular error thresholds, I measure the ratio of image pairs for
which the estimated displacement error falls below each threshold. The area
under this curve is called the mean Average Accuracy (mAA). The set of local
features is fixed (upright-SIFT) and I compare the mAA between a naive matching and
a box-constrained matching.</p>
<p>For one scene, the list of image pairs is the same as in the image-matching
challenge: image pairs are selected with various co-visibility i.e. the amount
of common structures in each image. The current plots are computed only on a
subset of these pairs for computational time considerations (until I speed up
the box matching). The list is made of the 100 pairs for which naive matching
performs the worst.</p>
<p align="center">
<img width="1024" src="/images/box_matching/stereo_plot.png">
</p>

<p align="center">
mAA over images pairs of the three phototourism validation scenes.
(reichstag and st-peters coming soon)
</p>

<h5>Feature-based Localization (coming soon)</h5>
<p>Given a set of reference images of a scene, the goal is to recover the relative
pose of a set of query images of the same scene with respect to the reference
ones. The reference images to construct the 3D of the scene and the query
images are then registered in the scene.</p>
<h5>Box detection</h5>
<p>The detection network is also evaluated independently with the standard
detection mAP (numerical results coming soon). Current experiments use a
Faster-RCNN architecture with a Resnet50 backbone.  Meanwhile, here is an image
example.
<p align="center">
<img width="784" src="/images/box_matching/reichstag_box.jpg">
</p></p>
<p align="center">
Qualitative results on the box detection network. Faster-RCNN with resnet50
backbone is trained on the reichstag scene. This image only
illustrates that such training can converge and can not be used to assess the
detection performance as the training and validation images are the same.
</p>

<h5>Data Annotation</h5>
<h6>Method</h6>
<p>Manual box annotations are collected over a subset of images depicting the same
scene with different viewpoints. The rest of the images are annotated by
projecting the manual annotations from the previous images using the image pose
and depth. The projection is run only between image pairs with similar
viewpoints to limit the effects of the depth noise. Given a reference image
with manual box annotations and a new image to annotate, the projection is
performed as follow: all the non-occluded pixels inside the manual box are
projected to the new image. The new box is the minimum enclosing box over the
projected points.</p>
<p align="center">
<img width="1100" src="/images/box_matching/box_annotation.png">
</p>

<p align="center">
Box annotation improvement. Left: Naive projection. The corners of the manual
boxes are projected to the new image. This can result in loose boxes and ignore
occlusions.
Right: Improved projection. All the points inside a box are projected to the
second image and the resulting box is the minimum enclosing box over the
majority of projected points.
</p>

<h4>Updates:</h4>
<ul>
<li>2020-06-12: Update box annotation propagation to make them tighter around the
  object of interest and to take into account the occlusion.</li>
<li>2020-05-27: Update stereo-matching metrics plot.
  The coarse homography is estimated from putative box matches based on their
  semantic labels and their visual similarity measured with the HardNet
  descriptor. At most 2 matches are kept for each box. Boxes around windows are
  discarded. Degenerate homographies are discarded early in the process.
  The computational time has been speeded up at the cost of less confidence on
  the homography (this needs to be quantified). Next step: Identify edge cases
  for the stereo-matching.</p></li>
<li>2020-05-15: Update stereo-matching metrics plot with full sacre-coeur
  results. The box matching is being improved to reduce the number of putative
  box matches that causes major latency by adding constraints on the boxes'
  geometric layout (e.g. putative matching boxes should form a simple polygon
  in both images).</p></li>
<li>2020-05-10: first commit</li></li>
</ul>
<h4>TODO:</h4>
<ol>
<li><s>Speed up the box matching.</s> One way to do so is to reduce the initial set of
  putative matches or to define a ransac threshold above which the estimated coarse
  image transformation is satisfying.</li>
<li><s>Integrate visual similarity when sampling putative match.</s>
  Refine the putative box matching: currently, boxes are matched based on their
  semantic consistency only which leads to a high number of matches. It is
  possible to add a visual similarity constraint between boxes with the same
  label by computing a descriptor over the box and match them only if the
  descriptor distance is below a threshold. Tune this threshold.</li>
<li>Identify stereo-matching edge cases and solve them.</li>
<li>Implement the localization evaluation.</li>
<li>Define the box annotation granularity i.e. to which architectural specificity
  the annotations go down to?</li>
<li>Manually annotate the boxes</li>
</ol>
<!--#### Setup

#### Dataset
Annotation progress on the [Phototourism](https://vision.uvic.ca/imw-challenge/index.md) dataset.

| Scene name              | Split | Size  | Annotated  |
| ----------------------- | :---: | :---: | :--------: |
| reichstag               | val   | 75    |     x     |   
| sacre_coeur             | val   | 1179  |     x     |
| st_peters_square        | val   | 2504  |     x     |
| brandenburg_gate        | train | 1363  |           |
| buckingham_palace       | train | 1676  |           |
| colosseum_exterior      | train | 2063  |           |
| grand_place_brussels    | train | 1083  |           |
| hagia_sophia_interior   | train | 889   |           |
| notre_dame_front_facade | train | 3765  |           |
| palace_of_westminster   | train | 983   |           |
| pantheon_exterior       | train | 1401  |           |
| prague_old_town_square  | train | 2316  |           |
| taj_mahal               | train | 1312  |           |
| temple_nara_japan       | train | 904   |           |
| trevi_fountain          | train | 3191  |           |
| westminster_abbey       | train | 1061  |           |-->
    </div><!-- /.entry-content -->
  </div>


  </article>
</section>

<script type="text/javascript">
    var disqus_shortname = 'abenbihi-github-io';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'https://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
</body>
</html>